{
  env_name: "LunarLander-v2",

  has_continuous_action_space: True, # continuous action space; else discrete

  max_ep_len: 1000, # max timesteps in one episode
  # break training loop if timeteps > max_training_timesteps
  max_training_timesteps: 10000000, # 10M

  # print avg reward in the interval (in num timesteps)
  print_freq: 10000, # max_ep_len * 10
  # log avg reward in the interval (in num timesteps)
  log_freq: 2000, # max_ep_len * 2,
  # save model frequency (in num timesteps)
  save_model_freq: 100000,

  # starting std for action distribution (         )
  action_std: 0.6,
  # linearly decay action_std (action_std = action_std - action_std_decay_rate)
  action_std_decay_rate: 0.05,
  # minimum action_std (stop decay after action_std <= min_action_std)
  min_action_std: 0.1,
  # action_std decay frequency (in num timesteps)
  action_std_decay_freq: 250000,
  #####################################################

  # Note : print/log frequencies should be > than max_ep_len

  ################ PPO hyperparameters ################
  update_timestep: 4000, # max_ep_len * 4, # update policy every n timesteps
  K_epochs: 80, # update policy for K epochs in one PPO update

  eps_clip: 0.2, # clip parameter for PPO
  gamma: 0.99, # discount factor

  lr_actor: 0.0003, # learning rate for actor network
  lr_critic: 0.001, # learning rate for critic network

  random_seed: 0, # set random seed if required (0 = no random seed)
}
